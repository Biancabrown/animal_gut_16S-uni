{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLA\n",
    "\n",
    "## Notes for this notebook\n",
    "\n",
    "* Running `LLA` on the `georg` sequencing run.\n",
    "* The `tony` sequencing run will be processed in a separate notebook, then the data will be merged.\n",
    "\n",
    "## General notes\n",
    "\n",
    "Ley Lab Amplicon pipeline (LLA)\n",
    "\n",
    "This notebook describes the standard operating procedure for generating sequence variants (SVs) from raw amplicon sequence data.\n",
    "\n",
    "***\n",
    "\n",
    "**This notebook is set up to process the following data:**\n",
    "\n",
    "* demultiplexed MiSeq paired-end sequences (raw fastq files)\n",
    "\n",
    "If your data differs from this, you may need to heavily modify this notebook in order to process your amplicon data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sequence data generated at the MPI Genome Center is already demultiplexed. The sequence data is stored at the [local SRA](http://sra.eb.local/sra/index.html). You should **symlink** all of your sequence data (the `fastq.gz` files) into 1 directory.\n",
    "\n",
    "An example for creating a symlink: \n",
    "\n",
    "`ln -s /path/to/MY_FILE.fastq.gz /new/path/to/MY_FILE.fastq.gz`\n",
    "\n",
    "**Note:** the directory should ONLY contain the read files that you want to import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of read files\n",
    "\n",
    "Your read files should all be in the same directory. QIIME2 will automatically find all read files in the directory. If you need to combine read files from multiple locations, it is best in most cases to create symlinks instead of copying the read files (this creates unnesessary redundant file copies). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:02.929524Z",
     "start_time": "2017-12-04T13:24:02.916573Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "read_file_dir = '/ebio/abt3_projects/Georg_animal_feces/data/16S/LLA/georg/'\n",
    "georg_read_dir = '/ebio/abt3_projects/Georg_animal_feces/data/16S/raw_run_data/georg/qiime2_import/'\n",
    "georg_read1_file = os.path.join(georg_read_dir, 'forward.fastq.gz')\n",
    "georg_read2_file = os.path.join(georg_read_dir, 'reverse.fastq.gz')\n",
    "georg_index_file = os.path.join(georg_read_dir, 'barcodes.fastq.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working directory\n",
    "\n",
    "Where do you want to generate all output files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:03.788454Z",
     "start_time": "2017-12-04T13:24:03.783298Z"
    }
   },
   "outputs": [],
   "source": [
    "work_dir = '/ebio/abt3_projects/Georg_animal_feces/data/16S/LLA/georg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type\n",
    "\n",
    "In the particular case of the mock community reads, the data type that fits our needs is `SampleData[PairedEndSequencesWithQuality]`. You may also have paired-end sequencing data with barcoded samples. In that case, the data type to chose is `EMPPairedEndSequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:04.352464Z",
     "start_time": "2017-12-04T13:24:04.348205Z"
    }
   },
   "outputs": [],
   "source": [
    "data_type = 'SampleData[PairedEndSequencesWithQuality]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "Database used for taxonomy classification. The default here is `Silva 119 99% OTUs from 515F/806R region of sequences`. See the [QIIME2 docs](https://docs.qiime2.org/2017.10/data-resources/#taxonomy-classifiers-for-use-with-q2-feature-classifier) for info on other databases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:04.800465Z",
     "start_time": "2017-12-04T13:24:04.795909Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = '/ebio/abt3_projects/databases/leylab16s/classifiers/silva-119-99-515-806-nb-classifier.qza'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cluster parameters\n",
    "\n",
    "These will be the default parameters used for all compute cluster jobs (SGE jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:05.760469Z",
     "start_time": "2017-12-04T13:24:05.749105Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAKE SURE to CHANGE this to your email!\n",
    "email='nyoungblut@tuebingen.mpg.de'\n",
    "\n",
    "# only used for multi-threaded steps\n",
    "threads=20   \n",
    "\n",
    "# Note: memory is per-thread (eg., 8 threads x 8G memory = 64G total memory)\n",
    "memory='3G'\n",
    "\n",
    "# Max job time \n",
    "job_time='24:0:0'\n",
    "\n",
    "# Output for SGE job errors/warnings/info\n",
    "SGE_out='~/SGE/LLA/'\n",
    "\n",
    "# Path to conda installation\n",
    "conda_env_path='/ebio/abt3_projects/software/miniconda3' \n",
    "\n",
    "# Particular conda environment to use\n",
    "conda_env='qiime2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**You shouldn't need to modify anything below this line**\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "\n",
    "Importing python packages, defining functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:07.840559Z",
     "start_time": "2017-12-04T13:24:07.834574Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:08.128486Z",
     "start_time": "2017-12-04T13:24:08.116343Z"
    }
   },
   "outputs": [],
   "source": [
    "# making directories (if they don't exist)\n",
    "\n",
    "## working directory\n",
    "work_dir = os.path.abspath(os.path.expanduser(work_dir))\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "## SGE output    \n",
    "SGE_out = os.path.abspath(os.path.expanduser(SGE_out))\n",
    "if not os.path.isdir(os.path.split(SGE_out)[0]):\n",
    "    os.makedirs(SGE_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:08.992550Z",
     "start_time": "2017-12-04T13:24:08.982208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ebio/abt3_projects/Georg_animal_feces/data/16S/LLA/georg\n"
     ]
    }
   ],
   "source": [
    "# changing directory\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Defining python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:10.684466Z",
     "start_time": "2017-12-04T13:24:10.676032Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(s, file_name):\n",
    "    \"\"\"Writing out (multi-line) string to file\n",
    "    \"\"\"\n",
    "    F = os.path.abspath(os.path.expanduser(file_name))\n",
    "    with open(F, 'w') as outF:\n",
    "        outF.write(s)\n",
    "    print('File written: {}'.format(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:11.480524Z",
     "start_time": "2017-12-04T13:24:11.468863Z"
    }
   },
   "outputs": [],
   "source": [
    "def sge_job(job_name='LLA', threads=threads, email=email, \n",
    "            memory=memory, job_time=job_time, SGE_out=SGE_out,\n",
    "            conda_env_path=conda_env_path, \n",
    "            conda_env=conda_env):\n",
    "    \"\"\"Creating an SGE job script template \n",
    "    \"\"\"\n",
    "    job = \"\"\"#!/bin/bash\n",
    "#$ -N {job_name}\n",
    "#$ -pe parallel {threads}\n",
    "#$ -l h_vmem={memory}\n",
    "#$ -l h_rt={job_time}\n",
    "#$ -o {SGE_out}\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "#$ -m ea\n",
    "#$ -M {email}\n",
    "\n",
    "CONDA_INSTALLATION=\"{conda_env_path}\"\n",
    "QIIME2_ENV=\"{conda_env}\"\n",
    "\n",
    "export PATH=\"$CONDA_INSTALLATION/bin\":$PATH\n",
    "export PATH=\"$CONDA_INSTALLATION/envs/$QIIME2_ENV/bin\":$PATH\n",
    "export LC_ALL=C.UTF-8\n",
    "export LANG=C.UTF-8\n",
    "\n",
    "\"\"\".format(job_name=job_name, threads=threads, email=email,\n",
    "           memory=memory, job_time=job_time, SGE_out=SGE_out,\n",
    "           conda_env_path=conda_env_path, conda_env=conda_env)\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:12.432552Z",
     "start_time": "2017-12-04T13:24:12.412489Z"
    }
   },
   "outputs": [],
   "source": [
    "def qsub_wait(file_name):\n",
    "    \"\"\"Submit SGE job via `qsub` then wait for the job to finish (success or abort)\n",
    "    \"\"\"\n",
    "    # submit job\n",
    "    ret = !! qsub $file_name\n",
    "    # get job ID\n",
    "    job_ID = ret[0].split(' ')[2]\n",
    "    print('SGE Job ID: {}'.format(job_ID))\n",
    "    \n",
    "    # query job\n",
    "    while 1:\n",
    "        ret = !! qstat\n",
    "        IDs = [x.lstrip().split(' ')[0] for x in ret]\n",
    "        if job_ID in IDs:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            print('SGE job finished: {}'.format(job_ID))\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimmomatic\n",
    "\n",
    "Before we can run `dada2` to call sequence variants, we need to first determine which positions in the reads to trim off due to low quality score (on average across reads). `dada2` requires this information in order to call sequence variants correctly. \n",
    "\n",
    "One could look at the sequence quality score distribution plots generated by `qiime demux summarize`; however, this method is subjective and will differ among researchers. To keep the data processing pipeline consistent, we use `trimmomatic` to determine the position in the reads where average quality scores drop below a specific cutoff.\n",
    "\n",
    "To determine the cutoff positions with Trimmomatic, we will use the following script. It provides a summary of the lengths of the reads after being trimmed by using a sliding window approach. The script will generate read length cutoffs for both the forward and reverse reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating trimmomatic pipeline script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:13.956556Z",
     "start_time": "2017-12-04T13:24:13.947589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting subsample_trim.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile subsample_trim.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# user input\n",
    "if [ \"$#\" -lt 5 ]; then\n",
    "    echo \"Usage: trim.sh read_file_dir sample_size window_size qlty_threshold threads\"\n",
    "    echo \"Description: randomly subsample from all reads, then run trimmotic to get a quality score cutoff\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# ===== INPUT PARAMETERS ===== \n",
    "\n",
    "FASTQ_DIR=$1\n",
    "SAMPLE_SIZE=$2\n",
    "WINDOW_SIZE=$3\n",
    "QLTY_TRESHOLD=$4\n",
    "THREADS=$5\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "declare -a READS_FILES=(\"forward\" \"reverse\")\n",
    "\n",
    "for READS_TO_USE in \"${READS_FILES[@]}\"\n",
    "do\n",
    "    if [ $READS_TO_USE == 'forward' ]\n",
    "    then\n",
    "        read=\"forward.fastq.gz\"\n",
    "    elif [ $READS_TO_USE == 'reverse' ]\n",
    "    then\n",
    "        read=\"reverse.fastq.gz\"\n",
    "    fi\n",
    "    \n",
    "    # Random subsampling\n",
    "    [ -e merged_${READS_TO_USE}.fastq ] && rm merged_${READS_TO_USE}.fastq\n",
    "    for filename in \"$FASTQ_DIR/$read\"; do\n",
    "        seqtk sample -s11 ${filename} ${SAMPLE_SIZE} >> merged_${READS_TO_USE}.fastq\n",
    "    done\n",
    "\n",
    "    # Trimming of the reads\n",
    "    [ -e trimmed_${READS_TO_USE}.fastq ] && rm trimmed_${READS_TO_USE}.fastq\n",
    "    [ -e trimmed_${READS_TO_USE}.log ] && rm trimmed_${READS_TO_USE}.log\n",
    "    trimmomatic SE \\\n",
    "        merged_${READS_TO_USE}.fastq \\\n",
    "        trimmed_${READS_TO_USE}.fastq \\\n",
    "        SLIDINGWINDOW:${WINDOW_SIZE}:${QLTY_TRESHOLD} -trimlog trimmed_${READS_TO_USE}.log -threads $THREADS\n",
    "    \n",
    "    # Recover the surviving sequence length of the trimmed reads\n",
    "    [ -e trim_results_${READS_TO_USE}.txt ] && rm trim_results_${READS_TO_USE}.txt\n",
    "    awk -F ' ' '{print $(NF-3)}' trimmed_${READS_TO_USE}.log >> trim_results_${READS_TO_USE}.txt\n",
    "\n",
    "    [ -e trim_median_${READS_TO_USE}.txt ]  && rm trim_median_${READS_TO_USE}.txt\n",
    "    sort -n trim_results_${READS_TO_USE}.txt | \\\n",
    "        awk '{ count[NR]=$1; } END { \\\n",
    "        if (NR % 2) { print count[(NR + 1) / 2]; } \\\n",
    "        else { print (count[(NR / 2)] + count[(NR / 2) + 1]) / 2.0; } }' > trim_median_${READS_TO_USE}.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the SGE job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:14.900476Z",
     "start_time": "2017-12-04T13:24:14.892319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: /ebio/abt3_projects/Georg_animal_feces/data/16S/LLA/georg/trim.sh\n"
     ]
    }
   ],
   "source": [
    "cmd = sge_job(job_name='trim') \n",
    "cmd = cmd + \"\"\"\n",
    "bash subsample_trim.sh {read_file_dir} {sample_size} {window_size} {qlty_threshold} {threads}\n",
    "\"\"\"\n",
    "cmd = cmd.format(read_file_dir=georg_read_dir, \n",
    "                 sample_size=10000, \n",
    "                 window_size=20, \n",
    "                 qlty_threshold=25, \n",
    "                 threads=threads)\n",
    "\n",
    "write_file(cmd, 'trim.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:24:15.304526Z",
     "start_time": "2017-12-04T13:24:15.186143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#$ -N trim\r\n",
      "#$ -pe parallel 20\r\n",
      "#$ -l h_vmem=3G\r\n",
      "#$ -l h_rt=24:0:0\r\n",
      "#$ -o /ebio/abt3/nyoungblut/SGE/LLA\r\n",
      "#$ -j y\r\n",
      "#$ -cwd\r\n",
      "#$ -m ea\r\n",
      "#$ -M nyoungblut@tuebingen.mpg.de\r\n",
      "\r\n",
      "CONDA_INSTALLATION=\"/ebio/abt3_projects/software/miniconda3\"\r\n",
      "QIIME2_ENV=\"qiime2\"\r\n",
      "\r\n",
      "export PATH=\"$CONDA_INSTALLATION/bin\":$PATH\r\n",
      "export PATH=\"$CONDA_INSTALLATION/envs/$QIIME2_ENV/bin\":$PATH\r\n",
      "export LC_ALL=C.UTF-8\r\n",
      "export LANG=C.UTF-8\r\n",
      "\r\n",
      "\r\n",
      "bash subsample_trim.sh /ebio/abt3_projects/Georg_animal_feces/data/16S/raw_run_data/georg/qiime2_import/ 10000 20 25 20\r\n"
     ]
    }
   ],
   "source": [
    "# view the job script\n",
    "!cat trim.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:31:25.364608Z",
     "start_time": "2017-12-04T13:24:15.502564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGE Job ID: 706956\n",
      "SGE job finished: 706956\n"
     ]
    }
   ],
   "source": [
    "# submit to the cluster and wait until job completion/abort\n",
    "qsub_wait('trim.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Sequence Variant selection with DADA2\n",
    "\n",
    "**No more OTU picking! No more 97% OTUs!**\n",
    "\n",
    "[DADA2](https://www.ncbi.nlm.nih.gov/pubmed/27214047) is a pipeline for detecting and correcting (where possible) Illumina amplicon sequence data, generating amplicon sequence variants (ASV or SV). The `dada2 denoised-paired` method requires two parameters that are used in quality filtering: `--p-trunc-len-f n` and `--p-trunc-len-r m` which truncate both the forward and reverse reads at the indicated positions `n` and `m`; this allows the user to remove low quality regions of the sequences. In our case, `n = m`, although you may select different values for `n` and `m`.\n",
    "\n",
    "To determine the values to pass for these two parameters, you should review the *Interactive Quality Plot* tab in the [QIIME2 Viewer](https://view.qiime2.org/) for the summary file (`demux.qzv`) obtained before. You can also chose to use the standard value that the Leylab 16S pipeline determines.\n",
    "\n",
    "**Note:** `dada2` may require *many hours* to finish, depending on the number of samples, number of threads, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:31:25.388497Z",
     "start_time": "2017-12-04T13:31:25.366722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trimmomatic-defined read quality cutoff position\n",
    "fwd_read_len = !! cat trim_median_forward.txt\n",
    "fwd_read_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:31:25.400508Z",
     "start_time": "2017-12-04T13:31:25.390374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'174'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trimmomatic-defined read quality cutoff position\n",
    "rev_read_len = !! cat trim_median_reverse.txt\n",
    "rev_read_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T13:31:25.412480Z",
     "start_time": "2017-12-04T13:31:25.402406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: /ebio/abt3_projects/Georg_animal_feces/data/16S/LLA/georg/dada2.sh\n"
     ]
    }
   ],
   "source": [
    "cmd = sge_job(job_name='dada2') \n",
    "cmd = cmd + \"\"\"\n",
    "qiime dada2 denoise-paired \\\n",
    "    --i-demultiplexed-seqs georg_demux.qza \\\n",
    "    --p-trim-left-f 19 \\\n",
    "    --p-trim-left-r 20 \\\n",
    "    --p-trunc-len-f {fwd_read_len} \\\n",
    "    --p-trunc-len-r {rev_read_len} \\\n",
    "    --p-n-threads {threads} \\\n",
    "    --o-representative-sequences rep-seqs.qza \\\n",
    "    --o-table table.qza\n",
    "\"\"\"\n",
    "cmd = cmd.format(fwd_read_len=fwd_read_len[0], \n",
    "                 rev_read_len=rev_read_len[0],\n",
    "                 threads=threads)\n",
    "\n",
    "write_file(cmd, 'dada2.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:18.467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#$ -N dada2\r\n",
      "#$ -pe parallel 20\r\n",
      "#$ -l h_vmem=3G\r\n",
      "#$ -l h_rt=24:0:0\r\n",
      "#$ -o /ebio/abt3/nyoungblut/SGE/LLA\r\n",
      "#$ -j y\r\n",
      "#$ -cwd\r\n",
      "#$ -m ea\r\n",
      "#$ -M nyoungblut@tuebingen.mpg.de\r\n",
      "\r\n",
      "CONDA_INSTALLATION=\"/ebio/abt3_projects/software/miniconda3\"\r\n",
      "QIIME2_ENV=\"qiime2\"\r\n",
      "\r\n",
      "export PATH=\"$CONDA_INSTALLATION/bin\":$PATH\r\n",
      "export PATH=\"$CONDA_INSTALLATION/envs/$QIIME2_ENV/bin\":$PATH\r\n",
      "export LC_ALL=C.UTF-8\r\n",
      "export LANG=C.UTF-8\r\n",
      "\r\n",
      "\r\n",
      "qiime dada2 denoise-paired     --i-demultiplexed-seqs georg_demux.qza     --p-trim-left-f 19     --p-trim-left-r 20     --p-trunc-len-f 250     --p-trunc-len-r 174     --p-n-threads 20     --o-representative-sequences rep-seqs.qza     --o-table table.qza\r\n"
     ]
    }
   ],
   "source": [
    "# view job script\n",
    "!cat dada2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:19.252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGE Job ID: 706957\n"
     ]
    }
   ],
   "source": [
    "# submit to the cluster and wait until job completion/abort\n",
    "qsub_wait('dada2.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** if the step above fails, you may need to increase the memory designation for the compute cluster job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Visualize SV table\n",
    "\n",
    "At this stage, you will hopefully have artifacts (remember: this are files, not sources of experimental bias) containing the feature table and corresponding feature sequences. You can summarize them as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature table\n",
    "\n",
    "This step will generate visual and tabular summaries of the SV table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:24.762Z"
    }
   },
   "outputs": [],
   "source": [
    "cmd = sge_job(job_name='feature_table', threads=1) \n",
    "cmd = cmd + \"\"\"\n",
    "qiime feature-table summarize \\\n",
    "  --i-table table.qza \\\n",
    "  --o-visualization table.qzv\n",
    "\"\"\"\n",
    "\n",
    "write_file(cmd, 'feature_table.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:25.148Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize job script\n",
    "!cat feature_table.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:25.347Z"
    }
   },
   "outputs": [],
   "source": [
    "# submit to the cluster and wait until job completion/abort\n",
    "qsub_wait('feature_table.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rep seqs\n",
    "\n",
    "This step will generate tabular view of feature identifier to sequence mapping, including links to BLAST each sequence against the NCBI nt database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:26.597Z"
    }
   },
   "outputs": [],
   "source": [
    "cmd = sge_job(job_name='req_seqs', threads=1) \n",
    "cmd = cmd + \"\"\"\n",
    "qiime feature-table tabulate-seqs \\\n",
    "  --i-data rep-seqs.qza \\\n",
    "  --o-visualization rep-seqs.qzv\n",
    "\"\"\"\n",
    "\n",
    "write_file(cmd, 'req_seqs.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:26.756Z"
    }
   },
   "outputs": [],
   "source": [
    "# view the job script before submission\n",
    "!cat req_seqs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-04T13:24:26.868Z"
    }
   },
   "outputs": [],
   "source": [
    "# submit to the cluster and wait until job completion/abort\n",
    "qsub_wait('req_seqs.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting table is a summary of the SV selection results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SessionInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T09:10:30.700513Z",
     "start_time": "2017-12-05T09:10:30.686594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': 'ca5443062',\n",
      " 'commit_source': 'installation',\n",
      " 'default_encoding': 'UTF-8',\n",
      " 'ipython_path': '/ebio/abt3_projects/software/miniconda3/envs/qiime2/lib/python3.5/site-packages/IPython',\n",
      " 'ipython_version': '6.2.1',\n",
      " 'os_name': 'posix',\n",
      " 'platform': 'Linux-4.4.67-x86_64-with-debian-stretch-sid',\n",
      " 'sys_executable': '/ebio/abt3_projects/software/miniconda3/envs/qiime2/bin/python',\n",
      " 'sys_platform': 'linux',\n",
      " 'sys_version': '3.5.4 | packaged by conda-forge | (default, Aug 10 2017, '\n",
      "                '01:38:41) \\n'\n",
      "                '[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]'}\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "print(IPython.sys_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qiime2]",
   "language": "python",
   "name": "conda-env-qiime2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1209px",
    "left": "0px",
    "right": "2348px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
